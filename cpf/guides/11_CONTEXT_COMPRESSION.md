# Context Compression Guide - v4.7.0

**Version**: 4.7.0
**Last Updated**: 2025-11-13
**Purpose**: Comprehensive guide to achieving 50-80% context reduction through proven compression techniques
**Based On**: Anthropic research (2025) and state-of-the-art context engineering practices

---

## Table of Contents

1. [Introduction](#introduction)
2. [Why Context Compression Matters](#why-context-compression-matters)
3. [The Four-Strategy Framework](#the-four-strategy-framework)
4. [Strategy 1: JIT Context Loading (SELECT)](#strategy-1-jit-context-loading-select)
5. [Strategy 2: Tool Output Filtering (COMPRESS)](#strategy-2-tool-output-filtering-compress)
6. [Strategy 3: Context Editing (ISOLATE)](#strategy-3-context-editing-isolate)
7. [Strategy 4: Real-Time Monitoring (WRITE)](#strategy-4-real-time-monitoring-write)
8. [Best Practices](#best-practices)
9. [Common Patterns](#common-patterns)
10. [Troubleshooting](#troubleshooting)
11. [Examples](#examples)
12. [FAQ](#faq)

---

## Introduction

**Context compression** is the practice of minimizing token usage in AI agent sessions through intelligent loading, summarization, and pruning techniques.

### What You'll Learn

- How to reduce context usage by 50-80% (research-validated)
- Four proven compression strategies
- When to apply each technique
- How to track compression effectiveness
- Common pitfalls and how to avoid them

### Who This Is For

- Users working on large projects (>1000 lines)
- Sessions approaching context limits (>50%)
- Cost-conscious developers (token usage = cost)
- Anyone wanting longer, more productive sessions

---

## Why Context Compression Matters

### The Context Problem

**Context limits** are real:
- Claude Sonnet 4.5: 200K token context window
- Effective limit: ~150K tokens (75% threshold)
- Without compression: Hit limit in 2-3 hours of work

**What happens without compression**:
```
Session without compression:
- Hour 1: 45K tokens used (22.5%)
- Hour 2: 95K tokens used (47.5%)
- Hour 3: 140K tokens used (70.0%)
- Result: Forced checkpoint at 3 hours
```

**With compression**:
```
Session with compression:
- Hour 1: 15K tokens used (7.5%)
- Hour 2: 30K tokens used (15.0%)
- Hour 3: 50K tokens used (25.0%)
- Hour 4: 75K tokens used (37.5%)
- Hour 5: 100K tokens used (50.0%)
- Hour 6: 130K tokens used (65.0%)
- Result: Checkpoint at 6 hours (2x longer!)
```

### Research Foundation

**Anthropic 2025 Research**:
- Context editing enables 84% token reduction
- JIT loading reduces unnecessary context by 30-50%
- Tool output filtering saves significant tokens
- 100-turn dialogues possible with only 16% of original tokens

**Benefits**:
1. **Longer sessions**: 2-3x duration before checkpoint
2. **Lower costs**: 50-80% reduction in token usage
3. **Better UX**: Fewer interruptions, more flow
4. **Higher productivity**: More work per session

---

## The Four-Strategy Framework

Context compression uses four strategies (**Write, Select, Compress, Isolate**):

### 1. SELECT (JIT Loading)
**What**: Load only what you need, when you need it
**When**: Exploring codebase, finding files
**Impact**: 30-50% reduction vs pre-loading

### 2. COMPRESS (Tool Filtering)
**What**: Summarize verbose tool outputs
**When**: Tool output >1K tokens
**Impact**: 15-25% additional reduction

### 3. ISOLATE (Context Editing)
**What**: Prune old low-signal content
**When**: After completing tasks, every 5-10 operations
**Impact**: 10-15% additional reduction

### 4. WRITE (Monitoring)
**What**: Track compression effectiveness
**When**: Continuously, report in checkpoint box
**Impact**: Enables optimization and adjustment

**Combined Impact**: 50-80% total reduction (multiplicative effect)

---

## Strategy 1: JIT Context Loading (SELECT)

### Principle

**Load only what you need, when you need it**

### The Problem

**Pre-loading wastes context**:
```python
# BAD: Pre-loading approach
Read src/file1.py  # 8K tokens
Read src/file2.py  # 6K tokens
Read src/file3.py  # 7K tokens
Read src/file4.py  # 9K tokens
Read src/file5.py  # 12K tokens
# Total: 42K tokens loaded

# Then discover: Only needed file2.py
# Waste: 36K tokens (85% waste rate)
```

### The Solution

**JIT Loading: Discovery â†’ Load on demand**:
```python
# GOOD: JIT loading approach
Grep "class Authentication" src/
# Returns: file2.py (500 tokens)

Read src/file2.py  # 6K tokens
# Total: 6.5K tokens loaded (85% savings)
```

### When to Use JIT Loading

**Use JIT Loading for**:
- âœ… Exploring codebase (don't know exact location)
- âœ… Finding specific functionality
- âœ… Understanding project structure (start high-level)
- âœ… Identifying relevant files before reading

**Don't use JIT Loading if**:
- âŒ Already know exact file to modify (just read it)
- âŒ Need comprehensive analysis of all files
- âŒ Small project (<10 files)

### JIT Loading Pattern

**Step-by-step workflow**:

```
1. SEARCH FIRST (Don't read yet)
   - Use Grep: Find pattern in files
   - Use Glob: List files by pattern
   - Goal: Identify relevant files (100-500 tokens)

2. EVALUATE MATCHES (Narrow down)
   - Review search results
   - Identify most relevant file
   - Goal: Choose 1-3 files to load (0 tokens)

3. LOAD ON DEMAND (Only what's needed)
   - Read chosen files only
   - Goal: Load minimum necessary content (5-15K tokens)

4. REPEAT IF NEEDED (Iterative refinement)
   - If wrong file: Search again
   - If need more context: Load additional file
   - Goal: Just enough context, not more
```

### Examples

**Example 1: Finding Authentication Logic**

```
âŒ BAD (Pre-loading):
Read src/auth.py
Read src/users.py
Read src/session.py
Read src/api.py
Read src/database.py
Total: 45K tokens
Then discover: Authentication in auth.py

âœ… GOOD (JIT Loading):
Grep "def authenticate" src/
Returns: src/auth.py (line 42)
Read src/auth.py
Total: 8K tokens (82% savings)
```

**Example 2: Understanding Module Structure**

```
âŒ BAD (Pre-loading):
Read all 20 files in src/ module
Total: 160K tokens
Then decide which to modify

âœ… GOOD (JIT Loading):
1. Glob "src/**/*.py" (get list of files)
   Result: 20 files identified (1K tokens)
2. Grep "class Config" src/ (find config files)
   Result: src/config.py (500 tokens)
3. Read src/config.py summary (head -n 50)
   Result: Confirms this is target (2K tokens)
4. Read src/config.py full
   Result: Full content loaded (8K tokens)
Total: 11.5K tokens (93% savings vs 160K)
```

### Implementation Tips

**Use discovery tools effectively**:
- `Grep "pattern" path/` - Find text in files
- `Glob "path/**/*.py"` - List files by pattern
- `Read file.py limit 50` - Preview first 50 lines
- `Bash ls -R` - List directory structure

**Ask before reading**: "Do I need this content NOW, or am I speculating?"

**Load iteratively**: High-level first, then drill down as needed

---

## Strategy 2: Tool Output Filtering (COMPRESS)

### Principle

**Summarize verbose tool outputs before adding to context**

### The Problem

**Tool outputs can be very verbose**:
```
Grep results: 47 matches listed (12K tokens)
Test output: Full pytest log (25K tokens)
File listings: 83 files enumerated (8K tokens)
Log files: 500 lines of debug logs (15K tokens)

Total: 60K tokens from tool outputs
```

### The Solution

**Summarize outputs >1K tokens**:
```
Grep results: Pattern summary (500 tokens) - 96% reduction
Test output: Pass/fail summary (50 tokens) - 99.8% reduction
File listings: Grouped summary (400 tokens) - 95% reduction
Log files: Key errors extracted (1K tokens) - 93% reduction

Total: 2K tokens (97% reduction from 60K)
```

### When to Summarize

**Summarize when**:
- âœ… Tool output >1K tokens
- âœ… Repetitive/structured output (many similar lines)
- âœ… Already saw detail (user has visibility)
- âœ… Only need pattern/summary, not every line

**Don't summarize when**:
- âŒ Critical error messages (need full context)
- âŒ Unique content (can't compress safely)
- âŒ Small outputs <1K tokens (not worth it)
- âŒ Debugging (need exact output)

### Tool-Specific Patterns

**Grep Results** (>20 matches):
```
âŒ BAD: List all 47 matches
src/auth.py:15: def login(username, password):
src/auth.py:42: def logout(session):
[... 45 more lines ...]

âœ… GOOD: Summarize pattern
"Found 47 matches across 12 files:
 - auth.py (15 matches): login/logout/validate
 - user.py (10 matches): user management
 - session.py (8 matches): session handling
 - Other (14 matches): utility usage
Most relevant: auth.py for authentication logic"
```

**Test Results**:
```
âŒ BAD: Full pytest output (25K tokens)
============================= test session starts ==============================
[... 500 lines of output ...]

âœ… GOOD: Summarized results (50 tokens)
"Tests: 127/127 passed (100%). Coverage: 94%.
 Runtime: 3.2s. 0 failures, 0 errors."
```

**File Listings** (>10 files):
```
âŒ BAD: List all 83 files
src/auth/login.py
src/auth/logout.py
[... 81 more files ...]

âœ… GOOD: Group by directory
"Found 83 files in 7 directories:
 - src/auth/ (15): authentication
 - src/users/ (22): user management
 - src/api/ (18): API endpoints
 - tests/ (25): test suites
 [... other groups ...]"
```

**Log Files**:
```
âŒ BAD: Full log output (15K tokens)
[2025-11-13 10:00:01] DEBUG: Starting service
[2025-11-13 10:00:02] INFO: Connected to database
[... 500 more lines ...]

âœ… GOOD: Extract key information
"Log analysis (last 500 lines):
 - 3 ERROR messages (database timeout at 10:05:23)
 - 12 WARNING messages (rate limit approaching)
 - Service started successfully at 10:00:01
 - Key issue: Database timeout needs investigation"
```

### Implementation Tips

**Pattern for summarizing**:
```
1. Run tool (get output)
2. Evaluate size (>1K tokens?)
3. If yes: Create summary
   - Count totals (how many?)
   - Group by type/category
   - Extract key information
   - Reference full output location
4. Use summary in context, not full output
```

**Summarization template**:
```
"[Tool] results:
 - Total: [count] items found
 - Categories: [list groups]
 - Most relevant: [key finding]
 - Full output: [where to find if needed]"
```

---

## Strategy 3: Context Editing (ISOLATE)

### Principle

**Remove low-signal content from context as work progresses**

### The Problem

**Completed work accumulates in context**:
```
Hour 1: Current task (20K tokens)
Hour 2: Hour 1 complete (20K) + Hour 2 current (20K) = 40K
Hour 3: Hour 1 (20K) + Hour 2 (20K) + Hour 3 (20K) = 60K
Hour 4: 80K tokens (mostly old completed work)
```

### The Solution

**Compress completed work to summaries**:
```
Hour 1: Current task (20K tokens)
Hour 2: Hour 1 summary (100 tokens) + Hour 2 current (20K) = 20.1K
Hour 3: Hour 1+2 summary (200 tokens) + Hour 3 current (20K) = 20.2K
Hour 4: Hour 1+2+3 summary (300 tokens) + Hour 4 current (20K) = 20.3K

Result: Maintains ~20K context throughout session (vs 80K)
```

### What to Prune

**SHOULD prune** (low-signal content):
- âœ… Tool outputs from >5 operations ago
- âœ… Completed task details (compress to summary)
- âœ… Exploration dead-ends (didn't lead to action)
- âœ… Verbose error messages (after understanding cause)
- âœ… Redundant file contents (reference path instead)
- âœ… Old conversation history (compress to decisions)

**MUST NOT prune** (high-signal content):
- âŒ Current task context (actively working on)
- âŒ Recent tool outputs (last 3-5 operations)
- âŒ State tracking data (master_state.json, etc.)
- âŒ Uncommitted changes (need to remember)
- âŒ Critical error messages (still debugging)
- âŒ User instructions (original requirements)

### Context Editing Pattern

**When to apply context editing**:
- After completing a task or sub-task
- Every 5-10 operations (periodic pruning)
- At 50% context threshold (aggressive pruning)
- After failed exploration (remove dead ends)
- After understanding error (remove verbose logs)

**How to apply context editing**:
```
1. IDENTIFY completed work
   - What tasks are done?
   - What tool outputs are old?
   - What content is no longer relevant?

2. CREATE summaries
   - 2-3 sentence summary per completed task
   - Key outcomes only (what changed?)
   - Reference details (where to find if needed)

3. PRUNE old content
   - Remove full details
   - Keep only summaries
   - Preserve in logs (operation_log.txt)

4. VERIFY continuity
   - Can you still continue work?
   - Is critical context preserved?
   - Can you remember what you're doing?
```

### Examples

**Example 1: Completed Task Compression**

```
âŒ BAD (Keep everything - 43K tokens):
Operation 1: Read src/auth.py (8K)
Operation 2: Read tests/test_auth.py (6K)
Operation 3: Analyzed auth flow (4K)
Operation 4: Wrote login() function (2K)
Operation 5: Wrote tests (3K)
Operation 6: Ran tests (20K pytest output)
Total: 43K tokens in context

âœ… GOOD (Compressed summary - 100 tokens):
"Completed: Added login() to src/auth.py with JWT tokens.
 Tests: 12 new tests, all passing (100%). Coverage: 96%.
 Ready to commit."
Total: 100 tokens (99.7% reduction)
Full details preserved in: operation_log.txt
```

**Example 2: Exploration Compression**

```
âŒ BAD (Keep all exploration - 30K tokens):
Read src/auth.py - not the target
Read src/session.py - not the target
Read src/user.py - not the target
Read src/api.py - THIS is the target
[All 4 file contents in context: 30K tokens]

âœ… GOOD (Keep only relevant - 8K tokens):
"Searched for API endpoint logic.
 Found in: src/api.py (full content kept)
 Checked: auth.py, session.py, user.py (not relevant, discarded)"
[Only api.py kept: 8K tokens, 73% reduction]
```

### Implementation Tips

**Compression template**:
```
"[Task] completed:
 - What: [Brief description of what was done]
 - Files: [Which files changed, line count]
 - Tests: [Test results if applicable]
 - Status: [Ready to commit / Next steps]
Full details: [Where to find them]"
```

**Safety checks before pruning**:
1. Is this content truly complete? (not active work)
2. Is summary accurate? (captures key points)
3. Can I continue without it? (test by asking "what was I working on?")
4. Is it preserved elsewhere? (logs, git, checkpoints)

---

## Strategy 4: Real-Time Monitoring (WRITE)

### Principle

**Track compression effectiveness and adjust techniques**

### Why Monitoring Matters

**You can't improve what you don't measure**:
- Are compression techniques working?
- Which operations consume most context?
- Is compression ratio improving over time?
- When should you adjust techniques?

### Metrics to Track

**Core metrics**:
```json
{
  "compression_metrics": {
    "raw_context_estimate": 180000,
    "compressed_context": 45000,
    "compression_ratio": 0.75,
    "techniques_used": ["jit_loading", "tool_filtering", "context_editing"]
  }
}
```

**Calculations**:

1. **Raw Context Estimate** (without compression):
   - Files that could be read: 50 Ã— 8K = 400K
   - Tool outputs at full verbosity: 80K
   - Exploration dead-ends: 60K
   - Total: 540K tokens

2. **Compressed Context** (actual usage):
   - Files actually read (JIT): 6 Ã— 8K = 48K
   - Summarized tool outputs: 12K (vs 80K)
   - Exploration summaries: 2K (vs 60K)
   - Total: 62K tokens

3. **Compression Ratio**:
   - (1 - compressed/raw) = (1 - 62K/540K) = 0.885 = **88.5%**

### Enhanced Checkpoint Box

**Standard checkpoint box**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š STATE TRACKING CHECKPOINT (AUTOMATIC - RULES 14-17)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Operation logged: Read â†’ logs/operation_log.txt
âœ… State updated: master_state.json (timestamp: 23:55:00)
âœ… Context tracked: 62K tokens (31.0%)
âœ… Threshold check: SAFE
âœ… Git status: 19d4c8d
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Enhanced with compression stats (v4.7.0)**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š STATE TRACKING CHECKPOINT (AUTOMATIC - RULES 14-17)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Operation logged: Read â†’ logs/operation_log.txt
âœ… State updated: master_state.json (timestamp: 23:55:00)
âœ… Context tracked: 62K tokens (31.0%)
âœ… Threshold check: SAFE
âœ… Git status: 19d4c8d

ğŸ“¦ Compression Stats (RULE 22 - v4.7.0):
âœ… Raw context estimate: 540K tokens (without compression)
âœ… Compressed context: 62K tokens (actual usage)
âœ… Compression ratio: 88.5% (8.7:1 reduction)
âœ… Techniques used: JIT loading, tool filtering, context editing
âœ… Token savings: 478K tokens saved this session

Next threshold: 65% at 130K tokens
Operations this session: 42
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### When to Adjust

**Compression ratio <50%** (not compressing enough):
- Action: Increase tool output summarization
- Action: More aggressive context editing
- Action: Switch to JIT loading if pre-loading
- Goal: Get compression ratio above 50%

**Context growing >10K/operation** (inefficient):
- Action: Identify which tools are verbose
- Action: Add summarization to those tools
- Action: Question if operation truly needed
- Goal: Reduce to <5K tokens/operation

**Compression ratio >90%** (excellent):
- Action: Continue current techniques
- Action: Verify no critical context lost
- Action: Document what's working well
- Goal: Maintain effectiveness

### Implementation Tips

**Track in context_tracking.json**:
```json
{
  "estimated_tokens": 62000,
  "max_tokens": 200000,
  "usage_pct": 31.0,
  "compression_metrics": {
    "raw_context_estimate": 540000,
    "compressed_context": 62000,
    "compression_ratio": 0.885,
    "techniques_used": ["jit_loading", "tool_filtering", "context_editing"],
    "last_compression": "2025-11-13T23:55:00Z"
  }
}
```

**Review regularly**:
- Check compression stats in checkpoint box
- Compare current vs past sessions
- Identify patterns (which operations are expensive?)
- Adjust techniques based on data

---

## Best Practices

### 1. Start with JIT Loading

**Why**: Biggest impact (30-50% reduction), easiest to apply

**How**:
- Always Grep/Glob before Read
- Load files on demand, not speculatively
- Ask: "Do I need this NOW?"

### 2. Summarize Aggressively

**Why**: Tool outputs are often verbose and repetitive

**How**:
- Set threshold: Summarize if >1K tokens
- Focus on patterns, not details
- Keep full output in logs if needed

### 3. Prune Regularly

**Why**: Prevents accumulation of old content

**How**:
- After each completed task (immediate)
- Every 5-10 operations (periodic)
- At 50% threshold (aggressive)

### 4. Track Effectiveness

**Why**: Can't optimize without data

**How**:
- Monitor compression metrics
- Review in checkpoint box
- Adjust techniques based on results

### 5. Preserve Safety

**Why**: Don't lose critical context

**How**:
- Follow "What NOT to Prune" guidelines
- Keep recent content (last 3-5 operations)
- Preserve in logs and checkpoints
- Test continuity ("can I still work?")

---

## Common Patterns

### Pattern 1: Codebase Exploration

**Scenario**: Need to find where authentication logic lives

**Without compression** (Pre-loading approach):
```
1. Read all 20 files in src/ â†’ 160K tokens
2. Identify auth.py as target
3. Make changes
Total: 160K tokens used
```

**With compression** (JIT Loading):
```
1. Grep "authenticate" src/ â†’ 500 tokens
2. Review matches, identify auth.py
3. Read auth.py â†’ 8K tokens
4. Make changes
Total: 8.5K tokens used (95% savings)
```

### Pattern 2: Test-Driven Development

**Scenario**: Write tests, run them, fix failures

**Without compression**:
```
1. Write test code â†’ 3K tokens
2. Run tests (full output) â†’ 25K tokens
3. Fix code â†’ 2K tokens
4. Run tests again (full output) â†’ 25K tokens
Total: 55K tokens used
```

**With compression** (Tool Filtering):
```
1. Write test code â†’ 3K tokens
2. Run tests (summarized) â†’ 50 tokens
3. Fix code â†’ 2K tokens
4. Run tests (summarized) â†’ 50 tokens
Total: 5.1K tokens used (91% savings)
```

### Pattern 3: Multi-File Refactoring

**Scenario**: Refactor authentication across 5 files

**Without compression**:
```
1. Read 5 auth files â†’ 40K tokens
2. Analyze dependencies â†’ 10K tokens
3. Make changes to each file â†’ 15K tokens
4. Test (full output) â†’ 25K tokens
Total: 90K tokens used
```

**With compression** (All strategies):
```
1. Grep for auth dependencies â†’ 1K tokens
2. Read 3 relevant files (JIT) â†’ 24K tokens
3. Analyze (then compress to summary) â†’ Compressed to 500 tokens
4. Make changes â†’ 15K tokens
5. Test (summarized) â†’ 50 tokens
Total: 40.5K tokens used (55% savings)
```

---

## Troubleshooting

### Problem: Context still growing too fast

**Symptoms**:
- Context >10K per operation
- Hitting 50% threshold quickly
- Compression ratio <50%

**Solutions**:
1. Check which operations are expensive (review logs)
2. Apply tool output filtering to verbose tools
3. Increase context editing frequency
4. Verify JIT loading is being used (not pre-loading)
5. Consider if all operations are truly necessary

### Problem: Lost critical context after pruning

**Symptoms**:
- Can't remember what you were working on
- Missing details needed to continue
- Errors due to missing context

**Solutions**:
1. Review checkpoint files (restore if needed)
2. Check operation_log.txt (full details preserved)
3. Check git commits (code changes preserved)
4. Adjust pruning rules (be less aggressive)
5. Add to "What NOT to Prune" list

### Problem: Don't know if compression is working

**Symptoms**:
- No visibility into compression effectiveness
- Can't tell if techniques are helping
- No baseline to compare against

**Solutions**:
1. Check compression_metrics in context_tracking.json
2. Review compression stats in checkpoint box
3. Compare current vs past sessions
4. Calculate manually if needed
5. Document findings for future reference

### Problem: Techniques too complex to remember

**Symptoms**:
- Forgetting to apply compression
- Inconsistent application
- Confusion about when to use which technique

**Solutions**:
1. Start with ONE technique: JIT loading
2. Practice until it becomes habit
3. Add next technique gradually
4. Reference this guide when uncertain
5. Review RULE 22 in rules/CLAUDE.md for quick reference

---

## Examples

### Example 1: Building Authentication System

**Task**: Implement user authentication with JWT tokens

**Step 1: Find existing auth code** (JIT Loading)
```
Grep "class Auth" src/
# Results: src/auth.py, src/user.py (500 tokens)

Read src/auth.py
# Load: 8K tokens
Total so far: 8.5K tokens
```

**Step 2: Understand current implementation** (Tool Filtering)
```
Grep "def login" src/auth.py
# Results: 3 matches (summarized: 200 tokens)
Total so far: 8.7K tokens
```

**Step 3: Implement new JWT logic** (Direct work)
```
Write JWT token generation
Edit src/auth.py (add token_generate())
# Added: 2K tokens
Total so far: 10.7K tokens
```

**Step 4: Test implementation** (Tool Filtering)
```
Run: pytest tests/test_auth.py
# Output: 25K tokens (full pytest log)
# Summarize: "12 tests, 11 passed, 1 failed (test_token_expiry)"
# Summarized: 50 tokens
Total so far: 10.75K tokens (avoided 25K)
```

**Step 5: Fix failing test** (Context Editing)
```
# Previous work now complete
# Compress steps 1-3 to summary:
"Authentication context: Added JWT generation to src/auth.py.
 Tests: 11/12 passing, fixing test_token_expiry."
# Compressed: 100 tokens (was 10.7K)

Fix test_token_expiry
Run tests again (summarized): 50 tokens
Total: 100 + 50 = 150 tokens for completed work
```

**Result**:
- Without compression: ~50K tokens (10.7K + 2K + 25K + 2K + 25K)
- With compression: ~11K tokens (10.75K summarization + 150 final)
- **Savings: 78% reduction**

### Example 2: Bug Investigation

**Task**: Find and fix database connection timeout

**Step 1: Search logs** (JIT Loading)
```
Grep "ERROR" logs/app.log
# Results: 47 errors found
# Summarize: "Database timeout errors (23), API errors (15), Other (9)"
# Loaded: 500 tokens (avoided reading full 15K log file)
```

**Step 2: Investigate database code** (JIT Loading)
```
Grep "connect" src/database.py
# Results: 3 functions (connection, reconnect, close)
# Loaded: 300 tokens

Read src/database.py
# Loaded: 7K tokens
Total: 7.8K tokens
```

**Step 3: Identify issue** (Tool Filtering)
```
Found: No timeout parameter in connection config
Fix: Add timeout=30 to connection params
# Work: 500 tokens
Total: 8.3K tokens
```

**Step 4: Test fix** (Tool Filtering + Context Editing)
```
Run tests (summarized): "25 passed, 0 failed. Timeout added successfully."
# Added: 50 tokens

Compress steps 1-3 to summary:
"Bug fix: Added 30s timeout to database connection (src/database.py line 42).
 Tests: All passing. Ready to commit."
# Final: 150 tokens (was 8.3K)
```

**Result**:
- Without compression: ~35K tokens (15K log + 7K code + 8K analysis + 5K testing)
- With compression: ~8.5K tokens (7.8K discovery + 0.7K fix)
- **Savings: 76% reduction**

---

## FAQ

### Q: Is compression mandatory?

**A**: RULE 22 techniques are SHOULD (Tier 2), not MUST (Tier 1). Important to follow, but flexible application based on context.

### Q: Will compression lose critical information?

**A**: No, if you follow "What NOT to Prune" guidelines. All content is preserved in logs and git, just removed from active context.

### Q: How do I know if compression is working?

**A**: Check compression metrics in checkpoint box. Target: 50-80% compression ratio. Monitor context usage over time.

### Q: Can I apply techniques selectively?

**A**: Yes! Start with JIT loading (biggest impact), add others gradually. Not all-or-nothing.

### Q: What if I'm not sure whether to prune something?

**A**: When in doubt, keep it. Better to be conservative than lose critical context. Can always prune later.

### Q: How much time does compression add?

**A**: Minimal. Most techniques (JIT, filtering) are just better workflows. Context editing adds ~30 seconds per checkpoint.

### Q: Does compression work for small projects?

**A**: Less impact on small projects (<1000 lines). Focus on JIT loading and tool filtering only.

### Q: Can compression techniques conflict with other rules?

**A**: No. RULE 22 enhances RULE 10 (context management). All techniques align with existing rules.

### Q: What's the minimum compression ratio to target?

**A**: Target 50% minimum. Research shows 50-80% is achievable with consistent application.

### Q: How do I start using compression?

**A**: Start with Strategy 1 (JIT Loading). It's the easiest and has the biggest impact. Add others once JIT becomes habit.

---

## Summary

**Context compression achieves 50-80% token reduction** through four proven strategies:

1. **SELECT** (JIT Loading): Load only what you need, when you need it
2. **COMPRESS** (Tool Filtering): Summarize verbose outputs >1K tokens
3. **ISOLATE** (Context Editing): Prune old low-signal content
4. **WRITE** (Monitoring): Track effectiveness and adjust

**Benefits**:
- 2-3x longer sessions before checkpoint
- 50-80% lower token costs
- Better user experience (fewer interruptions)
- Industry-standard practices (Anthropic-validated)

**Getting Started**:
1. Read RULE 22 in rules/CLAUDE.md
2. Start with JIT Loading (biggest impact)
3. Add Tool Filtering once comfortable
4. Add Context Editing for maximum efficiency
5. Track metrics and optimize

**Resources**:
- RULE 22: rules/CLAUDE.md section 22
- Implementation proposal: docs/planning/V4.7.0_IMPLEMENTATION_PROPOSAL.md
- Gap analysis: docs/analysis/FRAMEWORK_GAP_ANALYSIS_20251113.md

---

**Document Status**: COMPLETE
**Version**: 4.7.0
**Last Updated**: 2025-11-13

*Generated with Claude Code - Context-Preserving Framework v4.7.0*
